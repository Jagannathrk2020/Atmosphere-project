{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started with LIME: A explainer for a black box model\n",
    "\n",
    "This notebook shows example of how LIME can be applied to different models with different kinds of data.\n",
    "\n",
    "LIME makes models more **trustfull** and **interpretabel**. \n",
    "\n",
    "Before running this notebook make sure you have all dependencies installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This code was created using TensorFlow version 1.4.1, probably TensorFlow >= v1.0 is fine.\n",
      "Your TensorFlow version: 1.5.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "print('This code was created using TensorFlow version 1.4.1, probably TensorFlow >= v1.0 is fine.')\n",
    "print('Your TensorFlow version:', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does it works?\n",
    "\n",
    "Intuitively, an explanation is a local linear approximation of the model's behaviour. While the model may be very complex globally, it is easier to approximate it around the vicinity of a particular instance. While treating the model as a black box, we perturb the instance we want to explain and learn a sparse linear model around it, as an explanation.\n",
    "\n",
    "The figure below illustrates the intuition for this procedure. The model's decision function is represented by the blue/pink background, and is clearly nonlinear. The bright red cross is the instance being explained (let's call it X). We sample instances around X, and weight them according to their proximity to X (weight here is indicated by size). We then learn a linear model (dashed line) that approximates the model well in the vicinity of X, but not necessarily globally. For more information, check [the paper](https://arxiv.org/abs/1602.04938).\n",
    "\n",
    "![](https://raw.githubusercontent.com/marcotcr/lime/master/doc/images/lime.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Census Dataset (Tabular data)\n",
    "\n",
    "We'll use the [Adult dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/adult/) from the 1990 US Census. Our task is to predict whether an individual has an income over $50,000 / year (a classification problem), based attributes such as their age and occupation. This is a generic problem with a variety of numeric and categorical attributes - which makes it useful for demonstration purposes.\n",
    "\n",
    "This code is based on the code present [here](https://github.com/random-forests/tensorflow-workshop/blob/master/examples/07_structured_data.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_train_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
    "census_test_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test'\n",
    "census_train_path = tf.contrib.keras.utils.get_file('census.train', census_train_url)\n",
    "census_test_path = tf.contrib.keras.utils.get_file('census.test', census_test_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of features in this dataset, **this makes hard to understand and explain the model output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\n",
    "  'age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "  'marital-status', 'occupation', 'relationship', 'race', 'gender',\n",
    "  'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\n",
    "  'income'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset using Pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes\n",
    "# 1) We provide the header from above.\n",
    "# 2) The test file has a line we want to disgard at the top, so we include the parameter 'skiprows=1'\n",
    "census_train = pd.read_csv(census_train_path, index_col=False, names=column_names) \n",
    "census_test = pd.read_csv(census_test_path, skiprows=1, index_col=False, names=column_names) \n",
    "\n",
    "# Drop any rows that have missing elements\n",
    "# There are other ways to handle missing data, but we'll\n",
    "# take the simplest approach here.\n",
    "census_train = census_train.dropna(how=\"any\", axis=0)\n",
    "census_test = census_test.dropna(how=\"any\", axis=0)\n",
    "\n",
    "# Separate the label we want to predict into its own object \n",
    "# At the same time, we'll convert it into true/false to fix the formatting error\n",
    "census_train_label = census_train.pop('income').apply(lambda x: \">50K\" in x)\n",
    "census_test_label = census_test.pop('income').apply(lambda x: \">50K\" in x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treat categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'workclass': array([' ?', ' Federal-gov', ' Local-gov', ' Never-worked', ' Private',\n",
      "       ' Self-emp-inc', ' Self-emp-not-inc', ' State-gov', ' Without-pay'],\n",
      "      dtype=object)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lb_make = LabelEncoder()\n",
    "census_train['workclass'] = lb_make.fit_transform(census_train['workclass'])\n",
    "census_test['workclass'] = lb_make.fit_transform(census_test['workclass'])\n",
    "\n",
    "categorical_names = {\n",
    "    'workclass': lb_make.classes_\n",
    "}\n",
    "\n",
    "print(categorical_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a simple classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_session_config': None, '_num_ps_replicas': 0, '_keep_checkpoint_every_n_hours': 10000, '_task_type': 'worker', '_save_summary_steps': 100, '_num_worker_replicas': 1, '_model_dir': 'graphs/linear3', '_master': '', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fcfa11a1978>, '_is_chief': True, '_task_id': 0, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_log_step_count_steps': 100, '_tf_random_seed': None, '_service': None, '_keep_checkpoint_max': 5}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into graphs/linear3/model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 22.18071\n",
      "INFO:tensorflow:global_step/sec: 293.109\n",
      "INFO:tensorflow:step = 101, loss = 14.63991 (0.346 sec)\n",
      "INFO:tensorflow:global_step/sec: 317.276\n",
      "INFO:tensorflow:step = 201, loss = 16.94721 (0.317 sec)\n",
      "INFO:tensorflow:global_step/sec: 331.611\n",
      "INFO:tensorflow:step = 301, loss = 15.154967 (0.304 sec)\n",
      "INFO:tensorflow:global_step/sec: 266.634\n",
      "INFO:tensorflow:step = 401, loss = 17.185287 (0.372 sec)\n",
      "INFO:tensorflow:global_step/sec: 337.162\n",
      "INFO:tensorflow:step = 501, loss = 20.42313 (0.301 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.465\n",
      "INFO:tensorflow:step = 601, loss = 22.653885 (0.458 sec)\n",
      "INFO:tensorflow:global_step/sec: 316.227\n",
      "INFO:tensorflow:step = 701, loss = 16.29358 (0.308 sec)\n",
      "INFO:tensorflow:global_step/sec: 339.85\n",
      "INFO:tensorflow:step = 801, loss = 18.642511 (0.294 sec)\n",
      "INFO:tensorflow:global_step/sec: 270.177\n",
      "INFO:tensorflow:step = 901, loss = 14.927464 (0.367 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into graphs/linear3/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 16.933943.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.linear.LinearClassifier at 0x7fcfa11a1780>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_train_input_fn(): \n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "        x=census_train,\n",
    "        y=census_train_label, \n",
    "        batch_size=32,\n",
    "        num_epochs=None, # Repeat forever\n",
    "        shuffle=True)\n",
    "\n",
    "def create_test_input_fn():\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "        x=census_test,\n",
    "        y=census_test_label, \n",
    "        num_epochs=1, # Just one epoch\n",
    "        shuffle=False) # Don't shuffle so we can compare to census_test_labels later\n",
    "\n",
    "feature_columns = []\n",
    "\n",
    "age = tf.feature_column.numeric_column('age')\n",
    "feature_columns.append(age)\n",
    "\n",
    "workclass = tf.feature_column.categorical_column_with_vocabulary_list('workclass',\n",
    "                                                                      vocabulary_list=[x for x in list(set(census_train['workclass']))])\n",
    "feature_columns.append(workclass)\n",
    "\n",
    "'''\n",
    "age_buckets = tf.feature_column.bucketized_column(\n",
    "    tf.feature_column.numeric_column('age'), \n",
    "    boundaries=[31, 46, 60, 75, 90] # specify the ranges\n",
    ")\n",
    "\n",
    "feature_columns.append(age_buckets)\n",
    "\n",
    "education = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"education\", [\n",
    "        \"Bachelors\", \"HS-grad\", \"11th\", \"Masters\", \"9th\",\n",
    "        \"Some-college\", \"Assoc-acdm\", \"Assoc-voc\", \"7th-8th\",\n",
    "        \"Doctorate\", \"Prof-school\", \"5th-6th\", \"10th\", \"1st-4th\",\n",
    "        \"Preschool\", \"12th\"\n",
    "    ])\n",
    "\n",
    "feature_columns.append(education)\n",
    "\n",
    "# A categorical feature with a possibly large number of values\n",
    "# and the vocabulary not specified in advance.\n",
    "native_country = tf.feature_column.categorical_column_with_hash_bucket('native-country', 1000)\n",
    "feature_columns.append(native_country)\n",
    "\n",
    "age_cross_education = tf.feature_column.crossed_column(\n",
    "    [age_buckets, education],\n",
    "    hash_bucket_size=int(1e4) # Using a hash is handy here\n",
    ")\n",
    "feature_columns.append(age_cross_education)\n",
    "'''\n",
    "\n",
    "train_input_fn = create_train_input_fn()\n",
    "estimator = tf.estimator.LinearClassifier(feature_columns, model_dir='graphs/linear3', n_classes=2)\n",
    "estimator.train(train_input_fn, steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-02-08-03:31:32\n",
      "INFO:tensorflow:Restoring parameters from graphs/linear3/model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-08-03:31:33\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.7673976, accuracy_baseline = 0.76377374, auc = 0.69676805, auc_precision_recall = 0.3723111, average_loss = 0.5125317, global_step = 1000, label/mean = 0.23622628, loss = 65.19163, prediction/mean = 0.20704338\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7673976,\n",
       " 'accuracy_baseline': 0.76377374,\n",
       " 'auc': 0.69676805,\n",
       " 'auc_precision_recall': 0.3723111,\n",
       " 'average_loss': 0.5125317,\n",
       " 'global_step': 1000,\n",
       " 'label/mean': 0.23622628,\n",
       " 'loss': 65.19163,\n",
       " 'prediction/mean': 0.20704338}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input_fn = create_test_input_fn()\n",
    "estimator.evaluate(test_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fn(x):\n",
    "    def create_test_input_fn():\n",
    "        return tf.estimator.inputs.numpy_input_fn(\n",
    "            x={'age': np.array(x[:, 0]), 'workclass': np.array(map(int, x[:, 1]))},\n",
    "            y=None, \n",
    "            num_epochs=1, # Just one epoch\n",
    "            shuffle=False) # Don't shuffle so we can compare to census_test_labels later\n",
    "\n",
    "    preds = []\n",
    "    tf_prediction = estimator.predict(create_test_input_fn())\n",
    "    for p in tf_prediction:\n",
    "        preds.append(p['probabilities'])\n",
    "    return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from graphs/linear3/model.ckpt-1000\n",
      "Example 0. Actual: 0, Predicted: 0\n",
      "Example 1. Actual: 0, Predicted: 0\n",
      "Example 2. Actual: 1, Predicted: 0\n",
      "Example 3. Actual: 1, Predicted: 0\n",
      "Example 4. Actual: 0, Predicted: 0\n",
      "Example 5. Actual: 0, Predicted: 0\n",
      "Example 6. Actual: 0, Predicted: 0\n",
      "Example 7. Actual: 1, Predicted: 0\n",
      "Example 8. Actual: 0, Predicted: 0\n",
      "Example 9. Actual: 0, Predicted: 0\n"
     ]
    }
   ],
   "source": [
    "# reinitialize the input function\n",
    "test_input_fn = create_test_input_fn()\n",
    "\n",
    "predictions = estimator.predict(test_input_fn)\n",
    "i = 0\n",
    "for prediction in predictions:\n",
    "    true_label = census_test_label[i]\n",
    "    predicted_label = prediction['class_ids'][0]\n",
    "    # Uncomment the following line to see probabilities for individual classes\n",
    "    # print(prediction) \n",
    "    print(\"Example %d. Actual: %d, Predicted: %d\" % (i, true_label, predicted_label))\n",
    "    i += 1\n",
    "    if i == 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "feature_names = ['age', 'workclass']\n",
    "\n",
    "train = census_train.filter(['age','workclass'], axis=1)\n",
    "test = census_test.filter(['age','workclass'], axis=1)\n",
    "\n",
    "categorical_features = [1]\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(train.values, \n",
    "                                                   feature_names=feature_names,\n",
    "                                                   class_names=['income < 50', 'income >= 50'],\n",
    "                                                   categorical_features=categorical_features, verbose=True, mode='classification',\n",
    "                                                   discretize_continuous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-0ab0bdce1a56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/lime/lime_tabular.py\u001b[0m in \u001b[0;36mexplain_instance\u001b[0;34m(self, data_row, predict_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[1;32m    269\u001b[0m         ).ravel()\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0myss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minverse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;31m# for classification, the model needs to provide a list of tuples - classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-ca8e0a10894d>\u001b[0m in \u001b[0;36mpredict_fn\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtf_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_test_input_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf_prediction\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'probabilities'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input_fn, predict_keys, hooks, checkpoint_path)\u001b[0m\n\u001b[1;32m    419\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_and_assert_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m       features, input_hooks = self._get_features_from_input_fn(\n\u001b[0;32m--> 421\u001b[0;31m           input_fn, model_fn_lib.ModeKeys.PREDICT)\n\u001b[0m\u001b[1;32m    422\u001b[0m       estimator_spec = self._call_model_fn(\n\u001b[1;32m    423\u001b[0m           features, None, model_fn_lib.ModeKeys.PREDICT, self.config)\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_get_features_from_input_fn\u001b[0;34m(self, input_fn, mode)\u001b[0m\n\u001b[1;32m    583\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_features_from_input_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0;34m\"\"\"Extracts the `features` from return values of `input_fn`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_input_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m     \u001b[0minput_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_input_fn\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    692\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/cpu:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/inputs/numpy_io.py\u001b[0m in \u001b[0;36minput_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m       \u001b[0mordered_dict_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_keys\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mordered_dict_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mordered_dict_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/inputs/numpy_io.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    131\u001b[0m       \u001b[0mordered_dict_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_keys\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mordered_dict_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mordered_dict_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "i = 10\n",
    "exp = explainer.explain_instance(test.values[i], predict_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a interpretable model to classify Fake News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
